{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first RNN model (The baseline model)\n",
    "\n",
    "For **Visualization and Recommendation system teams**\n",
    "\n",
    "Please first install following stuff to run this notebook:\n",
    "\n",
    "1) Pytorch\n",
    "\n",
    "pip3 install torch torchvision\n",
    "\n",
    "2) TorchText\n",
    "\n",
    "pip3 install torchtext\n",
    "\n",
    "3) Then English model\n",
    "\n",
    "sudo python3 -m spacy download en\n",
    "pip3 install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Using TorchText is very helpful in data preprocessing of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.field.Field object at 0x7ff84b78b908>\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "#import spacy\n",
    "SEED = 1234\n",
    "#import en_core_web_sm\n",
    "#spacy = en_core_web_sm.load()\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "#doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "FILENAME = data.Field()\n",
    "\n",
    "print (TEXT)\n",
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next I wrote custom class to include additional stuff for IMDB dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IMDB(data.Dataset):\n",
    "\n",
    "    urls = ['/home/usman/Downloads/aclImdb_v1.tar.gz']\n",
    "    name = 'imdb'\n",
    "    dirname = 'aclImdb'\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    def __init__(self, path, text_field, label_field, **kwargs):\n",
    "        \"\"\"Create an IMDB dataset instance given a path and fields.\n",
    "        Arguments:\n",
    "            path: Path to the dataset's highest level directory\n",
    "            text_field: The field that will be used for text data.\n",
    "            label_field: The field that will be used for label data.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        #print(text_field)\n",
    "        fname_data = data.RawField()\n",
    "        \n",
    "        fields = [('text', text_field), ('label', label_field), ('aname', fname_data)]\n",
    "        examples = []\n",
    "        #dddddddddd\n",
    "        for label in ['pos', 'neg']:\n",
    "            for fname in glob.iglob(os.path.join(path, label, '*.txt')):\n",
    "                with io.open(fname, 'r', encoding=\"utf-8\") as f:\n",
    "                    text = f.readline()\n",
    "                #print(fname)\n",
    "                #print(text)\n",
    "                \n",
    "                examples.append(data.Example.fromlist([text, label,fname.split('aclImdb')[1]], fields))\n",
    "                #print(examples[0].fname)\n",
    "                #mmmmmmmmmmmmm\n",
    "\n",
    "        super(IMDB, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, root='.data',\n",
    "               train='train', test='test', **kwargs):\n",
    "        \"\"\"Create dataset objects for splits of the IMDB dataset.\n",
    "        Arguments:\n",
    "            text_field: The field that will be used for the sentence.\n",
    "            label_field: The field that will be used for label data.\n",
    "            root: Root dataset storage directory. Default is '.data'.\n",
    "            train: The directory that contains the training examples\n",
    "            test: The directory that contains the test examples\n",
    "            Remaining keyword arguments: Passed to the splits method of\n",
    "                Dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        return super(IMDB, cls).splits(\n",
    "            root=root, text_field=text_field, label_field=label_field,\n",
    "            train=train, validation=None, test=test, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def iters(cls, batch_size=32, device=0, root='.data', vectors=None, **kwargs):\n",
    "        \"\"\"Create iterator objects for splits of the IMDB dataset.\n",
    "        Arguments:\n",
    "            batch_size: Batch_size\n",
    "            device: Device to create batches on. Use - 1 for CPU and None for\n",
    "                the currently active GPU device.\n",
    "            root: The root directory that contains the imdb dataset subdirectory\n",
    "            vectors: one of the available pretrained vectors or a list with each\n",
    "                element one of the available pretrained vectors (see Vocab.load_vectors)\n",
    "            Remaining keyword arguments: Passed to the splits method.\n",
    "        \"\"\"\n",
    "        TEXT = data.Field()\n",
    "        LABEL = data.Field(sequential=False)\n",
    "        \n",
    "        #usman123\n",
    "        train, test = cls.splits(TEXT, LABEL, root=root, **kwargs)\n",
    "\n",
    "        TEXT.build_vocab(train, vectors=vectors)\n",
    "        LABEL.build_vocab(train)\n",
    "        #yyyyyyyy\n",
    "        return data.BucketIterator.splits(\n",
    "            (train, test), batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchtext import datasets\n",
    "\n",
    "train_data, test_data = IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many examples are in each split by checking their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 25000\n",
      "Total testing examples: 25000\n",
      "{'text': ['The', 'Ali', 'G', 'character', 'works', 'brilliantly', 'within', 'the', 'confines', 'of', 'a', 'comedy', 'show', ',', 'but', 'as', 'a', 'movie', ',', 'it', 'does', \"n't\", 'work', 'in', 'the', 'same', 'way.<br', '/><br', \"/>Don't\", 'get', 'me', 'wrong', '-', 'this', 'is', 'a', 'very', 'funny', 'movie', ',', 'full', 'of', 'biting', ',', 'witty', 'dialogue', ',', 'that', 'caricatures', 'the', 'modern', 'British', 'chav', 'wonderfully', 'well', ',', 'whilst', 'providing', 'the', 'viewer', 'with', 'a', 'hilarious', ',', 'if', 'unrealistic', 'story.<br', '/><br', '/>One', 'problem', 'with', 'this', 'film', 'is', 'that', 'the', 'script', 'and', 'content', 'is', 'either', 'fantastically', 'brilliant', ',', 'or', 'it', \"'s\", 'embarrassing', 'to', 'watch', '.', 'When', 'I', 'say', 'embarrassing', ',', 'I', 'do', \"n't\", 'mean', 'funny', 'embarrassing', 'a', 'la', 'Office', 'or', 'Extras', ',', 'but', 'rather', ',', 'you', \"'ll\", 'wish', 'they', 'had', \"n't\", 'included', 'it', 'in', 'the', 'final', 'cut', '.', 'One', 'example', 'of', 'this', 'is', 'the', 'inclusion', 'of', 'a', 'music', 'video', 'after', 'the', 'film', 'has', 'ended', ',', 'to', 'the', 'tune', 'of', ',', '\"', 'This', 'is', 'how', 'we', 'do', 'it', '.', '\"', 'Whenever', 'I', 'watch', 'the', 'film', ',', 'I', 'stop', 'the', 'DVD', 'when', 'it', 'says', 'the', 'end', ',', 'and', 'leave', 'it', 'at', 'that.<br', '/><br', '/>Overall', ',', 'Ali', 'G', 'Indahouse', 'is', 'a', 'good', 'film', ',', 'worth', 'watching', 'a', 'couple', 'of', 'times', '.', 'The', 'script', 'is', 'enjoyable', 'to', 'an', 'extent', ',', 'and', 'there', 'are', 'no', 'issues', 'as', 'far', 'as', 'acting', 'goes', '.', 'However', ',', 'refinement', 'is', 'the', 'key', 'word', 'here.<br', '/><br', '/>Ali', 'G', 'is', 'a', 'better', 'television', 'programme', '.', 'Borat', 'is', 'a', 'better', 'film', '.'], 'aname': '/test/pos/3496_7.txt', 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print('Total training examples:', len(train_data))\n",
    "print('Total testing examples:', len(test_data))\n",
    "print(vars(test_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example to help you access the training and testing objects.\n",
    "\n",
    "For **Viualization and Recommendation system Teams**:\n",
    "you can use these objects for your working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting training data into 75/25 train/valid split.\n",
    "\n",
    "For **Recommendation system Teams**:\n",
    "you can use these objects for your working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 17500\n",
      "Total validation examples:  7500\n",
      "Total testing examples:  25000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "print('Total training examples:', len(train_data))\n",
    "print('Total validation examples: ',len(valid_data))\n",
    "print('Total testing examples: ',len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now preparing data for the baseline NLP model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "#FILENAME.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['<', 'br', '/><br', '/>Dull', 'Demi', ',', 'going', 'thru', 'the', 'motions', '.', 'Ditto', 'Prochnow', '.', 'Ominous', 'portents', 'that', 'elicit', 'yawns', '.', 'Michael', 'Biehn', 'trying', 'to', 'be', 'dynamic', ',', 'which', 'ai', \"n't\", 'his', 'shtick.<br', '/><br', '/>To', 'quote', 'Buffy', 'Summers', ',', '\"', 'If', 'the', 'apocalypse', 'comes', '...', 'beep', 'me', '.', '\"<br', '/><br', '/>Going', 'back', 'to', 'sleep', 'now', '.'], 'aname': '/train/neg/7944_2.txt', 'label': 'neg'}\n",
      "{'batch_size_fn': None, 'train': False, 'random_shuffler': <torchtext.data.utils.RandomShuffler object at 0x7ff7b83e9f60>, '_iterations_this_epoch': 0, 'dataset': <__main__.IMDB object at 0x7ff7cd8f4358>, 'shuffle': False, 'iterations': 0, 'sort_within_batch': True, 'batch_size': 64, 'device': device(type='cpu'), 'repeat': False, 'sort': True, '_restored_from_state': False, '_random_state_this_epoch': None, 'sort_key': <function IMDB.sort_key at 0x7ff86002e488>}\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(vars(train_data.examples[0]))\n",
    "#usman:check iters\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)\n",
    "print(vars(test_iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The baseline Model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "       \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    " \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))\n",
    "    \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "#util funtions\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    #batch = next(iterator.__iter__())\n",
    "    #print(batch.text)\n",
    "    #print(batch.aname)\n",
    "    #print(batch.text)\n",
    "    #usman: check eval\n",
    "    \n",
    "    for batch in iterator:\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    #sssssssss\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            #print(predictions)\n",
    "            #print(batch.text.shape)\n",
    "            #print(TEXT.vocab.itos[11])\n",
    "            #sssdsdssd\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_test(model, iterator, criterion):\n",
    "    \n",
    "    \n",
    "    import csv\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    #sssssssss\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            rounded_preds_us = torch.round(torch.sigmoid(predictions))\n",
    "            \n",
    "            #print(predictions)\n",
    "            #print(batch.text.shape)\n",
    "            #print(TEXT.vocab.itos[11])\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            #print(predictions)\n",
    "            #print(rounded_preds_us)\n",
    "            #usman:check valid later\n",
    "            \n",
    "            with open('sentiment_results_file_baseline.csv', mode='a') as sent_file:\n",
    "                file_writer = csv.writer(sent_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                \n",
    "                for x in range(0,len(predictions)):\n",
    "                    file_writer.writerow([batch.aname[x], rounded_preds_us[x].item(),batch.label[x].item()])\n",
    "\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a3a326a9eb7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-b9e25122a514>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'baselineModel.pt')\n",
    "    \n",
    "    print('Epoch: ',str(epoch+1),' | Epoch Time:',str(epoch_mins),'m ',str(epoch_secs),'s')\n",
    "    print('\\tTrain Loss: ',str(train_loss),' | Train Acc: ',str(train_acc*100),'%')\n",
    "    print('\\t Val. Loss: ',str(valid_loss),' |  Val. Acc: ',str(valid_acc*100),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trainned model, now evalting on the testing 25,000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.7097292280258121  | Test Acc: 47.179507675683105 %\n",
      "Results are also written in the csv file\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('baselineModel.pt'))\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('sentiment_results_file_baseline.csv', mode='w') as sent_file:\n",
    "    file_writer = csv.writer(sent_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    file_writer.writerow(['Review File Path', 'Our Prediction', 'Ground Truth (1: pos, 0:neg)'])                \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_loss, test_acc = evaluate_test(model, test_iterator, criterion)\n",
    "\n",
    "print('Test Loss: ',str(test_loss),' | Test Acc:',str(test_acc*100),'%')\n",
    "print('Results are also written in the csv file')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
